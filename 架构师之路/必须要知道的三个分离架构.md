# Table of Contents

* [前言](#前言)
* [1  必备  前台与后台分离的架构实践](#1--必备--前台与后台分离的架构实践)
  * [1.1 业务场景介绍](#11-业务场景介绍)
  * [1.2 数据耦合的问题](#12-数据耦合的问题)
  * [1.3 系统耦合的问题](#13-系统耦合的问题)
  * [1.4 总结](#14-总结)
* [2  必备  动静分离架构实践](#2--必备--动静分离架构实践)
  * [2.1 静态页面](#21-静态页面)
  * [2.2 动态页面](#22-动态页面)
  * [2.3 互联网动静分离架构](#23-互联网动静分离架构)
  * [2.4 页面静态化](#24-页面静态化)
  * [2.5 页面静态化的应用场景](#25-页面静态化的应用场景)
  * [2.6 总结](#26-总结)
* [3  必备  读写分离架构实践](#3--必备--读写分离架构实践)
  * [3.1 什么是读写分离](#31-什么是读写分离)
  * [3.2 什么是数据库水平切分](#32-什么是数据库水平切分)
    * [3.2.1 按照范围水平拆分](#321-按照范围水平拆分)
    * [3.2.2 按照哈希水平拆分](#322-按照哈希水平拆分)
  * [3.3 分库分表和读写分离的案例分析](#33-分库分表和读写分离的案例分析)
  * [3.4 分库分表后如何生成全局唯一ID](#34-分库分表后如何生成全局唯一id)
    * [3.4.1 方案一独立数据库自增 id](#341-方案一独立数据库自增-id)
    * [3.4.2 UUID](#342-uuid)
    * [3.4.3 获取系统当前时间](#343-获取系统当前时间)
    * [3.4.4 SnowFlake 算法的思想分析](#344-snowflake-算法的思想分析)
  * [3.5 你的系统如何支撑高并发](#35-你的系统如何支撑高并发)
    * [3.5.1 一道面试题的背景引入](#351-一道面试题的背景引入)
    * [3.5.2 先考虑一个最简单的系统架构](#352-先考虑一个最简单的系统架构)
    * [3.5.3 系统集群化部署](#353-系统集群化部署)
    * [3.5.4 数据库分库分表 + 读写分离](#354-数据库分库分表--读写分离)
    * [3.5.5 缓存集群引入](#355-缓存集群引入)
    * [3.5.6 引入消息中间件集群](#356-引入消息中间件集群)
    * [3.5.7 现在能hold住高并发面试题了吗](#357-现在能hold住高并发面试题了吗)


# 前言

如果你经历过创业，经历过快速迭代业务，经历过用户量不断上涨，经历过访问并发越来越大，你一定会遇到以下系统问题：

- 用户访问页面越来越**慢**
- 系统性能下降，**数据库扛不住**，连接数经常打满，最终数据库挂掉，重启后又快速挂掉
- 改了一个小地方，另外一个看似不相干的地方却挂了，**严重耦合**

 

如果你没有经历过，很可能是：

- 没到这一步项目就死了
- 身在所谓的大公司，用着所谓先进的架构体系

 

创业初期遇到上述痛点，很容易想到“三个分离”的架构优化方案：

- **前后分离**：前台与后台的数据与访问分离
- **动静分离**：能够100倍以上的提升静态页面/资源的访问速度
- **读写分离**：能够快速的线性扩充数据库的读性能



# 1  必备  前台与后台分离的架构实践

## 1.1 业务场景介绍

虚拟一个类似于“安居客”租房买房的业务场景，这个业务的**数据有两大来源**：

- 用户发布的数据
- 爬虫从竞对抓取来的数据

 

这个业务对应的系统有**两类使用者**：

- 普通用户，浏览与发布数据，俗称“前台用户”
- 后台用户，运营与管理数据，俗称“后台用户”

![image-20190708094437401](http://ww3.sinaimg.cn/large/006tNc79ly1g4sgwja3o4j30aq05bt9s.jpg)

在一个创业公司，为了快速迭代，系统架构如上：

- **web层**：前台web，后台web
- **任务层**：抓取数据
- **数据层**：存储数据



## 1.2 数据耦合的问题

系统两类数据源，一类是用户发布的数据，一类是爬虫抓取的数据，两类**数据的特点不一样**：

- **自有**数据相对结构化，变化少
- **抓取**数据源很多，数据结构变化快

 

如果将自有数据和抓取数据耦合在一个库里，经常出现的情况是：

- -> 抓取数据结构变化
- -> 需要修改数据结构
- -> 影响前台用户展现
- -> 经常被动修改前台用户展现逻辑，配合抓取升级

如果经历过这个过程，其中的痛不欲生，是谁都不愿意再次回忆起的。



**优化思路**：前台展现数据，后台抓取数据分离，解耦。

![image-20190708094548673](http://ww2.sinaimg.cn/large/006tNc79ly1g4sgwjsxtej30bm04q3zf.jpg)

如上图所示：

- 前台展现的稳定数据，库独立
- 后台抓取的多变数据，库独立
- 任务层新增一个异步转换的任务

 

如此这般：

- 频繁变化的抓取程序，以及抓取的异构数据存储，解耦
- 前台数据与web都不需要被动配合升级
- 即使出现问题，前台用户的发布与展现都不影响



## 1.3 系统耦合的问题

上面解决了不同数据源写入的耦合问题，再来看看前台与后台用户访问的耦合问题。

 

**用户侧**，前台访问的**特点**是：

- 访问模式有限
- 访问量较大，DAU不达到百万都不好意思说是互联网C端产品
- 对访问时延敏感，用户如果访问慢，立马就流失了
- 对服务可用性要求高，系统经常用不了，用户还会再来么
- 对数据一致性的要求高，关乎用户体验的事情就是大事

 

**运营侧**，后台访问的**特点**是：

- 访问模式多种多样，运营销售各种奇形怪状的，大批量分页的，查询需求
- 用户量小，访问量小
- 访问延时不这么敏感，大批量分页，几十秒能出结果，也能接受
- 对可用性能容忍，系统挂了，10分钟之内重启能回复，也能接受
- 对一致性的要求始终，晚个30秒的数据，也能接受

![image-20190708094653470](http://ww4.sinaimg.cn/large/006tNc79ly1g4sgwkaex3j307504hmxg.jpg)

前台和后台的模式与访问需求都不一样，但是，如果前台与后台混用同一套服务和结构化数据，会导致：

- 后台的低性能访问，对前台用户产生巨大的影响，本质还是耦合

![image-20190708094713596](http://ww1.sinaimg.cn/large/006tNc79ly1g4sgwkpkjrj309a04gdge.jpg)

- 随着数据量变大，为了保证前台用户的时延，质量，做一些类似与分库分表的升级，数据库一旦变化，可能很多后台的需求难以满足

 

**优化思路**：冗余数据，前台与后台服务与数据分离，解耦。

![image-20190708094732950](http://ww3.sinaimg.cn/large/006tNc79ly1g4sgwlb3i9j30d704nab4.jpg)

如上图所示：

- 前台和后台独立服务与数据，解耦
- 如果出现问题，相互不影响

![image-20190708094823371](http://ww2.sinaimg.cn/large/006tNc79ly1g4sgwm702cj30du04kwfa.jpg)

通过不同的技术方案，在不同容忍度，业务对系统要求不同的情况下，可以使用不同的技术栈来满足各自的需求，如上图，后台使用ES或者hive在进行数据存储，用以满足“售各种奇形怪状的，大批量分页的，查询需求”



## 1.4 总结

创业初期，快速实施架构优化，提升性能的“三大分离”优化利器：

- **动静分离**：能够100倍以上的提升静态页面/资源的访问速度
- **读写分离**：能够快速的线性扩充数据库的读性能
- **前后分离**：前台与后台的数据与访问分离



备注：另外一种[前后端分离](./分层架构.md#4--互联网分层架构为啥要前后端分离)



# 2  必备  动静分离架构实践

## 2.1 静态页面

静态页面，是指互联网架构中，几乎不变的页面(或者变化频率很低)，例如：

- 首页等html页面
- js/css等样式文件
- jpg/apk等资源文件

![image-20190708095651421](http://ww3.sinaimg.cn/large/006tNc79ly1g4sgwmkyuyj309702n0t1.jpg)

静态页面，有与之匹配的技术架构来加速，例如：

- CDN
- nginx
- squid/varnish



## 2.2 动态页面

动态页面，是指互联网架构中，不同用户不同场景访问，都不一样的页面，例如：

- 百度搜索结果页
- 淘宝商品列表页
- 速运个人订单中心页

这些页面，不同用户，不同场景访问，大都会动态生成不同的页面。

![image-20190708100041357](http://ww2.sinaimg.cn/large/006tNc79ly1g4sgwn6ulzj306404a0t8.jpg)

动态页面，有与之匹配的技术架构，例如：

- 分层架构
- 服务化架构
- 数据库，缓存架构

 

## 2.3 互联网动静分离架构

动静分离是指，静态页面与动态页面分开不同系统访问的架构设计方法。

![image-20190708100206505](http://ww4.sinaimg.cn/large/006tNc79ly1g4sgwnjdzej30dv059myc.jpg)

一般来说：

- **静态页面**访问路径短，访问速度快，几毫秒
- **动态页面**访问路径长，访问速度相对较慢(数据库的访问，网络传输，业务逻辑计算)，几十毫秒甚至几百毫秒，对架构扩展性的要求更高
- 静态页面与动态页面以不同域名区分



## 2.4 页面静态化

既然静态页面访问快，动态页面生成慢，有没有可能，将原本需要动态生成的站点提前生成好，使用静态页面加速技术来访问呢？

这就是互联网架构中的**“页面静态化”**优化技术。



举例，如下图，58同城的帖子详情页，原本是需要动态生成的：

![image-20190708100552790](http://ww4.sinaimg.cn/large/006tNc79ly1g4sgwo247kj30a906ign5.jpg)

- 浏览器发起http请求，访问/detail/12348888x.shtml **详情页**
- web-server层从RESTful接口中，解析出帖子id是**12348888**
- service层通过DAO层拼装**SQL语句**，访问数据库
- 最终获取数据，**拼装html返回浏览器**



而“页面静态化”是指，将帖子ID为12348888的帖子12348888x.shtml提前生成好，由静态页面相关加速技术来加速：

![image-20190708100616283](http://ww1.sinaimg.cn/large/006tNc79ly1g4sgwoxfl7j30c703h0ti.jpg)

这样的话，将极大提升访问速度，减少访问时间，提高用户体验。



## 2.5 页面静态化的应用场景

页面静态化优化后速度会加快，那能不能所有的场景都使用这个优化呢？**哪些业务场景适合使用这个架构优化方案呢？**

 

**一切脱离业务的架构设计都是耍流氓**，页面静态化，适用于：总数据量不大，生成静态页面数量不多的业务。例如：

- 58速运的城市页只有几百个，就可以用这个优化，只需提前生成几百个城市的“静态化页面”即可
- 一些二手车业务，只有几万量二手车库存，也可以提前生成这几万量二手车的静态页面
- 像58同城这样的信息模式业务，有几十亿的帖子量，就**不**太适合于静态化（碎片文件多，反而访问慢）



## 2.6 总结

**“页面静态化”**是一种将原本需要动态生成的站点提前生成静态站点的优化技术。

总数据量不大，**生成静态页面数量不多的业务**，非常适合于“页面静态化”优化。



# 3  必备  读写分离架构实践

## 3.1 什么是读写分离

**什么是数据库读写分离？**

![image-20190708150431176](http://ww2.sinaimg.cn/large/006tNc79ly1g4sgwpxtpzj306w053gmj.jpg)

答：一主多从，读写分离，主动同步，是一种常见的数据库架构，一般来说：

- 主库，提供数据库写服务
- 从库，提供数据库读服务
- 主从之间，通过某种机制同步数据，例如mysql的binlog

一个组从同步集群通常称为一个**“分组”**。



**分组架构究竟解决什么问题？**

答：大部分互联网业务读多写少，数据库的读往往最先成为性能瓶颈，如果希望：

- 线性提升数据库读性能
- 通过消除读写锁冲突提升数据库写性能

此时可以使用分组架构。

 

一句话，分组主要解决“数据库读性能瓶颈”问题，在数据库扛不住读的时候，通常读写分离，通过增加从库线性提升系统读性能。



## 3.2 什么是数据库水平切分

**什么是数据库水平切分？**

![image-20190708151007615](http://ww2.sinaimg.cn/large/006tNc79ly1g4sgwqbe16j3071031dg8.jpg)

答：水平切分，也是一种常见的数据库架构，一般来说：

- 每个数据库之间没有数据重合，没有类似binlog同步的关联
- 所有数据并集，组成全部数据
- 会用算法，来完成数据分割，例如“取模”

一个水平切分集群中的每一个数据库，通常称为一个**“分片”**。



在数据量很大的情况下，数据层（缓存，数据库）涉及数据的水平扩展，将原本存储在一台服务器上的数据（缓存，数据库）水平拆分到不同服务器上去，以达到扩充系统性能的目的。

 

互联网数据层常见的水平拆分方式有这么几种，以数据库为例：

### 3.2.1 按照范围水平拆分

![image-20190708150724192](http://ww4.sinaimg.cn/large/006tNc79ly1g4sgwqujb5j3085039jrt.jpg)

每一个数据服务，存储一定范围的数据，上图为例：

- user0库，存储uid范围1-1kw
- user1库，存储uid范围1kw-2kw



这个方案的好处是：

（1）规则简单，service只需判断一下uid范围就能路由到对应的存储服务；

（2）数据均衡性较好；

（3）比较容易扩展，可以随时加一个uid[2kw,3kw]的数据服务；



不足是：

（1）请求的负载不一定均衡，一般来说，新注册的用户会比老用户更活跃，大range的服务请求压力会更大；

### 3.2.2 按照哈希水平拆分

![image-20190708150817619](http://ww3.sinaimg.cn/large/006tNc79ly1g4sgwr9gbjj308e0310t3.jpg)

每一个数据库，存储某个key值hash后的部分数据，上图为例：

- user0库，存储偶数uid数据
- user1库，存储奇数uid数据



这个方案的好处是：

（1）规则简单，service只需对uid进行hash能路由到对应的存储服务；

（2）数据均衡性较好；

（3）请求均匀性较好；



不足是：

（1）不容易扩展，扩展一个数据服务，hash方法改变时候，可能需要进行数据迁移；



## 3.3 分库分表和读写分离的案例分析

首先我们先考虑第一个问题，数据库每秒上万的并发请求应该如何来支撑呢？

要搞清楚这个问题，先得明白一般数据库部署在什么配置的服务器上。通常来说，假如你用普通配置的服务器来部署数据库，那也起码是 16 核 32G 的机器配置。

这种非常普通的机器配置部署的数据库，**一般线上的经验是：不要让其每秒请求支撑超过 2000，一般控制在 2000 左右。**

控制在这个程度，一般数据库负载相对合理，不会带来太大的压力，没有太大的宕机风险。

所以首先第一步，就是在上万并发请求的场景下，部署个 5 台服务器，每台服务器上都部署一个数据库实例。

然后每个数据库实例里，都创建一个一样的库，比如说订单库。此时在 5 台服务器上都有一个订单库，名字可以类似为：db_order_01、db_order_02 等等。

然后每个订单库里，都有一个相同的表，比如说订单库里有订单信息表，那么此时 5 个订单库里都有一个订单信息表。

比如：db_order_01 库里就有一个 tb_order_01 表，db_order_02 库里就有一个 tb_order_02 表。

这就实现了一个基本的分库分表的思路，原来的一台数据库服务器变成了 5 台数据库服务器，原来的一个库变成了 5 个库，原来的一张表变成了 5 个表。

然后你在写入数据的时候，需要借助数据库中间件，比如 Sharding-JDBC，或者是 MyCAT，都可以。

你可以根据比如订单 ID 来 Hash 后按 5 取模，比如每天订单表新增 50 万数据，此时其中 10 万条数据会落入 db_order_01 库的 tb_order_01 表，另外 10 万条数据会落入 db_order_02 库的 tb_order_02 表，以此类推。

这样就可以把数据均匀分散在 5 台服务器上了，查询的时候，也可以通过订单ID 来 hash 取模，去对应的服务器上的数据库里，从对应的表里查询那条数据出来即可。



依据这个思路画出的图如下所示，大家可以看看：

![image-20190708151435919](http://ww4.sinaimg.cn/large/006tNc79ly1g4sgwrpse3j30i4073whv.jpg)

做这一步有什么好处呢？第一个好处，原来比如订单表就一张表，这个时候不就成了 5 张表了么，那么每个表的数据就变成 1/5 了。

假设订单表一年有 1 亿条数据，此时 5 张表里每张表一年就 2000 万数据了。

那么假设当前订单表里已经有 2000 万数据了，此时做了上述拆分，每个表里就只有 400 万数据了。

而且每天新增 50 万数据的话，那么每个表才新增 10 万数据，这样是不是初步缓解了单表数据量过大影响系统性能的问题？

另外就是每秒 1 万请求到 5 台数据库上，每台数据库就承载每秒 2000 的请求，是不是一下子把每台数据库服务器的并发请求降低到了安全范围内？

这样，降低了数据库的高峰期负载，同时还保证了高峰期的性能。



**大量分表来保证海量数据下的查询性能**
但是上述的数据库架构还有一个问题，那就是单表数据量还是过大，现在订单表才分为了 5 张表，那么如果订单一年有 1 亿条，每个表就有 2000 万条，这也还是太大了。

所以还应该继续分表，大量分表。比如可以把订单表一共拆分为 1024 张表，这样 1 亿数据量的话，分散到每个表里也就才 10 万量级的数据量，然后这上千张表分散在 5 台数据库里就可以了。

在写入数据的时候，需要做两次路由，先对订单 ID Hash 后对数据库的数量取模，可以路由到一台数据库上，然后再对那台数据库上的表数量取模，就可以路由到数据库上的一个表里了。

通过这个步骤，就可以让每个表里的数据量非常小，每年 1 亿数据增长，但是到每个表里才 10 万条数据增长，这个系统运行 10 年，每个表里可能才百万级的数据量。

这样可以一次性为系统未来的运行做好充足的准备，看下面的图，一起来感受一下：

![image-20190708151515758](http://ww3.sinaimg.cn/large/006tNc79ly1g4sgws9b4lj30jj08w787.jpg)



**读写分离来支撑按需扩容以及性能提升**
这个时候整体效果已经挺不错了，大量分表的策略保证可能未来 10 年，每个表的数据量都不会太大，这可以保证单表内的 SQL 执行效率和性能。

然后多台数据库的拆分方式，可以保证每台数据库服务器承载一部分的读写请求，降低每台服务器的负载。

但是此时还有一个问题，假如说每台数据库服务器承载每秒 2000 的请求，然后其中 400 请求是写入，1600 请求是查询。

也就是说，增删改的 SQL 才占到了 20% 的比例，80% 的请求是查询。此时假如说随着用户量越来越大，又变成每台服务器承载 4000 请求了。

那么其中 800 请求是写入，3200 请求是查询，如果说你按照目前的情况来扩容，就需要增加一台数据库服务器。

但是此时可能就会涉及到表的迁移，因为需要迁移一部分表到新的数据库服务器上去，是不是很麻烦？

其实完全没必要，数据库一般都支持读写分离，也就是做主从架构。

写入的时候写入主数据库服务器，查询的时候读取从数据库服务器，就可以让一个表的读写请求分开落地到不同的数据库上去执行。

这样的话，假如写入主库的请求是每秒 400，查询从库的请求是每秒 1600。

那么图大概如下所示：

![image-20190708151825873](http://ww4.sinaimg.cn/large/006tNc79ly1g4sgwsopujj30jo0epagv.jpg)

写入主库的时候，会自动同步数据到从库上去，保证主库和从库数据一致。

然后查询的时候都是走从库去查询的，这就通过数据库的主从架构实现了读写分离的效果了。

现在的好处就是，假如说现在主库写请求增加到 800，这个无所谓，不需要扩容。然后从库的读请求增加到了 3200，需要扩容了。

这时，你直接给主库再挂载一个新的从库就可以了，两个从库，每个从库支撑 1600 的读请求，不需要因为读请求增长来扩容主库。

实际上线上生产你会发现，读请求的增长速度远远高于写请求，所以读写分离之后，大部分时候就是扩容从库支撑更高的读请求就可以了。

而且另外一点，对同一个表，如果你既写入数据（涉及加锁），还从该表查询数据，可能会牵扯到锁冲突等问题，无论是写性能还是读性能，都会有影响。

所以一旦读写分离之后，对主库的表就仅仅是写入，没任何查询会影响他，对从库的表就仅仅是查询。



## 3.4 分库分表后如何生成全局唯一ID

在分库分表之后你必然要面对的一个问题，就是 id 咋生成？因为要是一个表分成多个表之后，每个表的 id 都是从 1 开始累加自增长，那肯定不对啊。

举个例子，你的订单表拆分为了 1024 张订单表，每个表的 id 都从 1 开始累加，这个肯定有问题了！

你的系统就没办法根据表主键来查询订单了，比如 id = 50 这个订单，在每个表里都有！

所以此时就需要分布式架构下的全局唯一 id 生成的方案了，在分库分表之后，对于插入数据库中的核心 id，不能直接简单使用表自增 id，要全局生成唯一 id，然后插入各个表中，保证每个表内的某个 id，全局唯一。

比如说订单表虽然拆分为了 1024 张表，但是 id = 50 这个订单，只会存在于一个表里。

那么如何实现全局唯一 id 呢？有以下几种方案：

### 3.4.1 方案一独立数据库自增 id

这个方案就是说你的系统每次要生成一个 id，都是往一个独立库的一个独立表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。

比如说你有一个 auto_id 库，里面就一个表，叫做 auto_id 表，有一个 id 是自增长的。

那么你每次要获取一个全局唯一 id，直接往这个表里插入一条记录，获取一个全局唯一 id 即可，然后这个全局唯一 id 就可以插入订单的分库分表中。

这个方案的好处就是方便简单，谁都会用。缺点就是单库生成自增 id，要是高并发的话，就会有瓶颈的，因为 auto_id 库要是承载个每秒几万并发，肯定是不现实的了。

### 3.4.2 UUID

这个每个人都应该知道吧，就是用 UUID 生成一个全局唯一的 id。

好处就是每个系统本地生成，不要基于数据库来了。不好之处就是，UUID 太长了，作为主键性能太差了，不适合用于主键。

如果你是要随机生成个什么文件名了，编号之类的，你可以用 UUID，但是作为主键是不能用 UUID 的。

### 3.4.3 获取系统当前时间

这个方案的意思就是获取当前时间作为全局唯一的 id。但是问题是，并发很高的时候，比如一秒并发几千，会有重复的情况，这个肯定是不合适的。

一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个 id，如果业务上你觉得可以接受，那么也是可以的。

你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号，比如说订单编号：时间戳 + 用户 id + 业务含义编码。

### 3.4.4 SnowFlake 算法的思想分析

SnowFlake 算法，是 Twitter 开源的分布式 id 生成算法。其核心思想就是：使用一个 64 bit 的 long 型的数字作为全局唯一 id。

这 64 个 bit 中，其中 1 个 bit 是不用的，然后用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id，12 bit 作为序列号。

![image-20190708152912751](http://ww3.sinaimg.cn/large/006tNc79ly1g4sgwtolmlj30jy04x74s.jpg)

给大家举个例子吧，比如下面那个 64 bit 的 long 型数字：

- 第一个部分，是 1 个 bit：0，这个是无意义的。 
- 第二个部分是 41 个 bit：表示的是时间戳。
- 第三个部分是 5 个 bit：表示的是机房 id，10001。
- 第四个部分是 5 个 bit：表示的是机器 id，1 1001。
- 第五个部分是 12 个 bit：表示的序号，就是某个机房某台机器上这一毫秒内同时生成的 id 的序号，0000 00000000。

**①1 bit：是不用的，为啥呢？**

因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。

**②41 bit：表示的是时间戳，单位是毫秒。**

41 bit 可以表示的数字多达 2^41 - 1，也就是可以标识 2 ^ 41 - 1 个毫秒值，换算成年就是表示 69 年的时间。

**③10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10 台机器上，也就是 1024 台机器。**

但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 2 ^ 5 个机房（32 个机房），每个机房里可以代表 2 ^ 5 个机器（32 台机器）。

**④12 bit：这个是用来记录同一个毫秒内产生的不同 id。**

12 bit 可以代表的最大正整数是 2 ^ 12 - 1 = 4096，也就是说可以用这个 12 bit 代表的数字来区分同一个毫秒内的 4096 个不同的 id。

简单来说，你的某个服务假设要生成一个全局唯一 id，那么就可以发送一个请求给部署了 SnowFlake 算法的系统，由这个 SnowFlake 算法系统来生成唯一 id。

这个 SnowFlake 算法系统首先肯定是知道自己所在的机房和机器的，比如机房 id = 17，机器 id = 12。

接着 SnowFlake 算法系统接收到这个请求之后，首先就会用二进制位运算的方式生成一个 64 bit 的 long 型 id，64 个 bit 中的第一个 bit 是无意义的。

接着 41 个 bit，就可以用当前时间戳（单位到毫秒），然后接着 5 个 bit 设置上这个机房 id，还有 5 个 bit 设置上机器 id。

最后再判断一下，当前这台机房的这台机器上这一毫秒内，这是第几个请求，给这次生成 id 的请求累加一个序号，作为最后的 12 个 bit。

最终一个 64 个 bit 的 id 就出来了，类似于：

![image-20190708152930908](http://ww1.sinaimg.cn/large/006tNc79ly1g4sgwu5pjuj30jw04w0t8.jpg)

这个算法可以保证说，一个机房的一台机器上，在同一毫秒内，生成了一个唯一的 id。可能一个毫秒内会生成多个 id，但是有最后 12 个 bit 的序号来区分开来。

下面我们简单看看这个 SnowFlake 算法的一个代码实现，这就是个示例，大家如果理解了这个意思之后，以后可以自己尝试改造这个算法。

总之就是用一个 64 bit 的数字中各个 bit 位来设置不同的标志位，区分每一个 id。

SnowFlake 算法的实现代码如下：

```java
public class IdWorker {
  private long workerId; // 这个就是代表了机器id
  private long datacenterId; // 这个就是代表了机房id
  private long sequence; // 这个就是代表了一毫秒内生成的多个id的最新序号
  public IdWorker(long workerId, long datacenterId, long sequence) {
    // sanity check for workerId
    // 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0
    if (workerId > maxWorkerId || workerId < 0) {

      throw new IllegalArgumentException(
        String.format("worker Id can't be greater than %d or less than 0",maxWorkerId));
    }

    if (datacenterId > maxDatacenterId || datacenterId < 0) {

      throw new IllegalArgumentException(
        String.format("datacenter Id can't be greater than %d or less than 0",maxDatacenterId));
    }
    this.workerId = workerId;
    this.datacenterId = datacenterId;
    this.sequence = sequence;
  }
  private long twepoch = 1288834974657L;
  private long workerIdBits = 5L;
  private long datacenterIdBits = 5L;

  // 这个是二进制运算，就是5 bit最多只能有31个数字，也就是说机器id最多只能是32以内
  private long maxWorkerId = -1L ^ (-1L << workerIdBits); 
  // 这个是一个意思，就是5 bit最多只能有31个数字，机房id最多只能是32以内
  private long maxDatacenterId = -1L ^ (-1L << datacenterIdBits); 
  private long sequenceBits = 12L;
  private long workerIdShift = sequenceBits;
  private long datacenterIdShift = sequenceBits + workerIdBits;
  private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits;
  private long sequenceMask = -1L ^ (-1L << sequenceBits);
  private long lastTimestamp = -1L;
  public long getWorkerId(){
    return workerId;
  }
  public long getDatacenterId() {
    return datacenterId;
  }
  public long getTimestamp() {
    return System.currentTimeMillis();
  }
  // 这个是核心方法，通过调用nextId()方法，让当前这台机器上的snowflake算法程序生成一个全局唯一的id
  public synchronized long nextId() {
    // 这儿就是获取当前时间戳，单位是毫秒
    long timestamp = timeGen();
    if (timestamp < lastTimestamp) {
      System.err.printf(
        "clock is moving backwards. Rejecting requests until %d.", lastTimestamp);
      throw new RuntimeException(
        String.format("Clock moved backwards. Refusing to generate id for %d milliseconds",
               lastTimestamp - timestamp));
    }

    // 下面是说假设在同一个毫秒内，又发送了一个请求生成一个id
    // 这个时候就得把seqence序号给递增1，最多就是4096
    if (lastTimestamp == timestamp) {

      // 这个意思是说一个毫秒内最多只能有4096个数字，无论你传递多少进来，
      //这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围
      sequence = (sequence + 1) & sequenceMask; 
      if (sequence == 0) {
        timestamp = tilNextMillis(lastTimestamp);
      }

    } else {
      sequence = 0;
    }
    // 这儿记录一下最近一次生成id的时间戳，单位是毫秒
    lastTimestamp = timestamp;
    // 这儿就是最核心的二进制位运算操作，生成一个64bit的id
    // 先将当前时间戳左移，放到41 bit那儿；将机房id左移放到5 bit那儿；将机器id左移放到5 bit那儿；将序号放最后12 bit
    // 最后拼接起来成一个64 bit的二进制数字，转换成10进制就是个long型
    return ((timestamp - twepoch) << timestampLeftShift) |
        (datacenterId << datacenterIdShift) |
        (workerId << workerIdShift) | sequence;
  }
  private long tilNextMillis(long lastTimestamp) {

    long timestamp = timeGen();

    while (timestamp <= lastTimestamp) {
      timestamp = timeGen();
    }
    return timestamp;
  }
  private long timeGen(){
    return System.currentTimeMillis();
  }
  //---------------测试---------------
  public static void main(String[] args) {

    IdWorker worker = new IdWorker(1,1,1);

    for (int i = 0; i < 30; i++) {
      System.out.println(worker.nextId());
    }
  }
}
```

SnowFlake 算法一个小小的改进思路：其实在实际的开发中，这个SnowFlake算法可以做一点点改进。因为大家可以考虑一下，我们在生成唯一 id 的时候，一般都需要指定一个表名，比如说订单表的唯一 id。

所以上面那 64 个 bit 中，代表机房的那 5 个 bit，可以使用业务表名称来替代，比如用 00001 代表的是订单表。

因为其实很多时候，机房并没有那么多，所以那 5 个 bit 用做机房 id 可能意义不是太大。

这样就可以做到，SnowFlake 算法系统的每一台机器，对一个业务表，在某一毫秒内，可以生成一个唯一的 id，一毫秒内生成很多 id，用最后 12 个 bit 来区分序号对待。



## 3.5 你的系统如何支撑高并发

### 3.5.1 一道面试题的背景引入

这篇文章，我们聊聊大量同学问我的一个问题，面试的时候被问到一个让人特别手足无措的问题：你的系统如何支撑高并发？

大多数同学被问到这个问题压根儿没什么思路去回答，不知道从什么地方说起，其实本质就是没经历过一些真正有高并发系统的锤炼罢了。

因为没有过相关的项目经历，所以就没法从真实的自身体会和经验中提炼出一套回答，然后系统的阐述出来自己复杂过的系统如何支撑高并发的。

所以，这篇文章就从这个角度切入来简单说说这个问题，用一个最简单的思路来回答，大致如何应对。

当然这里首先说清楚一个前提：高并发系统各不相同。比如每秒百万并发的中间件系统、每日百亿请求的网关系统、瞬时每秒几十万请求的秒杀大促系统。

他们在应对高并发的时候，因为系统各自自身特点的不同，所以应对架构都是不一样的。

另外，比如电商平台中的订单系统、商品系统、库存系统，在高并发场景下的架构设计也是不同的，因为背后的业务场景什么的都不一样。

所以，这篇文章主要是给大家提供一个回答这类问题的思路，不涉及任何复杂架构设计，让你不至于在面试中被问到这个问题时，跟面试官大眼瞪小眼。

具体要真能在面试的时候回答好这个问题，建议各位参考一下本文思路，然后对你自己手头负责的系统多去思考一下，最好做一些相关的架构实践。



### 3.5.2 先考虑一个最简单的系统架构

假设刚刚开始你的系统就部署在一台机器上，背后就连接了一台数据库，数据库部署在一台服务器上。

我们甚至可以再现实点，给个例子，你的系统部署的机器是4核8G，数据库服务器是16核32G。

此时假设你的系统用户量总共就10万，用户量很少，日活用户按照不同系统的场景有区别，我们取一个较为客观的比例，10%吧，每天活跃的用户就1万。

按照28法则，每天高峰期算他4个小时，高峰期活跃的用户占比达到80%，就是8000人活跃在4小时内。

然后每个人对你的系统发起的请求，我们算他每天是20次吧。那么高峰期8000人发起的请求也才16万次，平均到4小时内的每秒（14400秒），每秒也就10次请求。

好吧！完全跟高并发搭不上边，对不对？

然后系统层面每秒是10次请求，对数据库的调用每次请求都会好几次数据库操作的，比如做做crud之类的。

那么我们取一个一次请求对应3次数据库请求吧，那这样的话，数据库层每秒也就30次请求，对不对？

按照这台数据库服务器的配置，支撑是绝对没问题的。

上述描述的系统，用一张图表示，就是下面这样：

![image-20190708154141254](http://ww4.sinaimg.cn/large/006tNc79ly1g4shjk4ekpj30700a90th.jpg)



### 3.5.3 系统集群化部署

假设此时你的用户数开始快速增长，比如注册用户量增长了50倍，上升到了500万。

此时日活用户是50万，高峰期对系统每秒请求是500/s。然后对数据库的每秒请求数量是1500/s，这个时候会怎么样呢？

按照上述的机器配置来说，如果你的系统内处理的是较为复杂的一些业务逻辑，是那种重业务逻辑的系统的话，是比较耗费CPU的。

此时，4核8G的机器每秒请求达到500/s的时候，很可能你会发现你的机器CPU负载较高了。

然后数据库层面，以上述的配置而言，其实基本上1500/s的高峰请求压力的话，还算可以接受。

这个主要是要观察数据库所在机器的磁盘负载、网络负载、CPU负载、内存负载，按照我们的线上经验而言，那个配置的数据库在1500/s请求压力下是没问题的。

所以此时你需要做的一个事情，首先就是要支持你的系统集群化部署。

你可以在前面挂一个负载均衡层，把请求均匀打到系统层面，让系统可以用多台机器集群化支撑更高的并发压力。

比如说这里假设给系统增加部署一台机器，那么每台机器就只有250/s的请求了。

这样一来，两台机器的CPU负载都会明显降低，这个初步的“高并发”不就先cover住了吗？

要是连这个都不做，那单台机器负载越来越高的时候，极端情况下是可能出现机器上部署的系统无法有足够的资源响应请求了，然后出现请求卡死，甚至系统宕机之类的问题。

所以，简单小结，第一步要做的：

添加负载均衡层，将请求均匀打到系统层。 系统层采用集群化部署多台机器，扛住初步的并发压力。

此时的架构图变成下面的样子：

![image-20190708154231589](http://ww1.sinaimg.cn/large/006tNc79ly1g4shjkiza0j30ac0dsta5.jpg)



### 3.5.4 数据库分库分表 + 读写分离

假设此时用户量继续增长，达到了1000万注册用户，然后每天日活用户是100万。

那么此时对系统层面的请求量会达到每秒1000/s，系统层面，你可以继续通过集群化的方式来扩容，反正前面的负载均衡层会均匀分散流量过去的。

但是，这时数据库层面接受的请求量会达到3000/s，这个就有点问题了。

此时数据库层面的并发请求翻了一倍，你一定会发现线上的数据库负载越来越高。

每次到了高峰期，磁盘IO、网络IO、内存消耗、CPU负载的压力都会很高，大家很担心数据库服务器能否抗住。

没错，一般来说，对那种普通配置的线上数据库，建议就是读写并发加起来，按照上述我们举例的那个配置，不要超过3000/s。

因为数据库压力过大，首先一个问题就是高峰期系统性能可能会降低，因为数据库负载过高对性能会有影响。

另外一个，压力过大把你的数据库给搞挂了怎么办？

所以此时你必须得对系统做分库分表 + 读写分离，也就是把一个库拆分为多个库，部署在多个数据库服务上，这是作为主库承载写入请求的。

然后每个主库都挂载至少一个从库，由从库来承载读请求。

此时假设对数据库层面的读写并发是3000/s，其中写并发占到了1000/s，读并发占到了2000/s。

那么一旦分库分表之后，采用两台数据库服务器上部署主库来支撑写请求，每台服务器承载的写并发就是500/s。每台主库挂载一个服务器部署从库，那么2个从库每个从库支撑的读并发就是1000/s。

简单总结，并发量继续增长时，我们就需要focus在数据库层面：分库分表、读写分离。

此时的架构图如下所示：

![image-20190708154329317](http://ww4.sinaimg.cn/large/006tNc79ly1g4shjkzayfj30i40ggwh3.jpg)



### 3.5.5 缓存集群引入

接着就好办了，如果你的注册用户量越来越大，此时你可以不停的加机器，比如说系统层面不停加机器，就可以承载更高的并发请求。

然后数据库层面如果写入并发越来越高，就扩容加数据库服务器，通过分库分表是可以支持扩容机器的，如果数据库层面的读并发越来越高，就扩容加更多的从库。

但是这里有一个很大的问题：数据库其实本身不是用来承载高并发请求的，所以通常来说，数据库单机每秒承载的并发就在几千的数量级，而且数据库使用的机器都是比较高配置，比较昂贵的机器，成本很高。

如果你就是简单的不停的加机器，其实是不对的。

所以在高并发架构里通常都有缓存这个环节，缓存系统的设计就是为了承载高并发而生。

所以单机承载的并发量都在每秒几万，甚至每秒数十万，对高并发的承载能力比数据库系统要高出一到两个数量级。

所以你完全可以根据系统的业务特性，对那种写少读多的请求，引入缓存集群。

具体来说，就是在写数据库的时候同时写一份数据到缓存集群里，然后用缓存集群来承载大部分的读请求。

这样的话，通过缓存集群，就可以用更少的机器资源承载更高的并发。

比如说上面那个图里，读请求目前是每秒2000/s，两个从库各自抗了1000/s读请求，但是其中可能每秒1800次的读请求都是可以直接读缓存里的不怎么变化的数据的。

那么此时你一旦引入缓存集群，就可以抗下来这1800/s读请求，落到数据库层面的读请求就200/s。

同样，给大家来一张架构图，一起来感受一下：

![image-20190708154417915](http://ww2.sinaimg.cn/large/006tNc79ly1g4shjlfyi3j30ip0j8juf.jpg)

按照上述架构，他的好处是什么呢？

可能未来你的系统读请求每秒都几万次了，但是可能80%~90%都是通过缓存集群来读的，而缓存集群里的机器可能单机每秒都可以支撑几万读请求，所以耗费机器资源很少，可能就两三台机器就够了。

你要是换成是数据库来试一下，可能就要不停的加从库到10台、20台机器才能抗住每秒几万的读并发，那个成本是极高的。

好了，我们再来简单小结，承载高并发需要考虑的第三个点：

不要盲目进行数据库扩容，数据库服务器成本昂贵，且本身就不是用来承载高并发的 针对写少读多的请求，引入缓存集群，用缓存集群抗住大量的读请求



### 3.5.6 引入消息中间件集群

接着再来看看数据库写这块的压力，其实是跟读类似的。

假如说你所有写请求全部都落地数据库的主库层，当然是没问题的，但是写压力要是越来越大了呢？

比如每秒要写几万条数据，此时难道也是不停的给主库加机器吗？

可以当然也可以，但是同理，你耗费的机器资源是很大的，这个就是数据库系统的特点所决定的。

相同的资源下，数据库系统太重太复杂，所以并发承载能力就在几千/s的量级，所以此时你需要引入别的一些技术。

比如说消息中间件技术，也就是MQ集群，他是非常好的做写请求异步化处理，实现削峰填谷的效果。

假如说，你现在每秒是1000/s次写请求，其中比如500次请求是必须请求过来立马写入数据库中的，但是另外500次写请求是可以允许异步化等待个几十秒，甚至几分钟后才落入数据库内的。

那么此时你完全可以引入消息中间件集群，把允许异步化的每秒500次请求写入MQ，然后基于MQ做一个削峰填谷。比如就以平稳的100/s的速度消费出来然后落入数据库中即可，此时就会大幅度降低数据库的写入压力。

ps：关于MQ削峰填谷的概念，在公众号之前讲消息中间件的文章中已详细阐述，如果大伙儿忘记了，可以回顾一下。

此时，架构图变成了下面这样：

![image-20190708154516683](http://ww1.sinaimg.cn/large/006tNc79ly1g4shjmg8f1j30hy0g4wh6.jpg)

大家看上面的架构图，首先消息中间件系统本身也是为高并发而生，所以通常单机都是支撑几万甚至十万级的并发请求的。

所以，他本身也跟缓存系统一样，可以用很少的资源支撑很高的并发请求，用他来支撑部分允许异步化的高并发写入是没问题的，比使用数据库直接支撑那部分高并发请求要减少很多的机器使用量。

而且经过消息中间件的削峰填谷之后，比如就用稳定的100/s的速度写数据库，那么数据库层面接收的写请求压力，不就成了500/s + 100/s = 600/s了么？

大家看看，是不是发现减轻了数据库的压力？

到目前为止，通过下面的手段，我们已经可以让系统架构尽可能用最小的机器资源抗住了最大的请求压力，减轻了数据库的负担。

系统集群化 数据库层面的分库分表+读写分离 针对读多写少的请求，引入缓存集群 针对高写入的压力，引入消息中间件集群，

初步来说，简单的一个高并发系统的阐述是说完了。

但是，其实故事到这里还远远没有结束。



### 3.5.7 现在能hold住高并发面试题了吗

看完了这篇文章，你觉得自己能回答好面试里的高并发问题了吗？

很遗憾，答案是不能。而且我觉得单单凭借几篇文章是绝对不可能真的让你完全回答好这个问题的，这里有很多原因在里面。

首先，高并发这个话题本身是非常复杂的，远远不是一些文章可以说的清楚的，他的本质就在于，真实的支撑复杂业务场景的高并发系统架构其实是非常复杂的。

比如说每秒百万并发的中间件系统、每日百亿请求的网关系统、瞬时每秒几十万请求的秒杀大促系统、支撑几亿用户的大规模高并发电商平台架构，等等。

为了支撑高并发请求，在系统架构的设计时，会结合具体的业务场景和特点，设计出各种复杂的架构，这需要大量底层技术支撑，需要精妙的架构和机制设计的能力。

最终，各种复杂系统呈现出来的架构复杂度会远远超出大部分没接触过的同学的想象。

如果大家想要看一下有一定发复杂度的系统的架构设计和演进过程，可以看一下之前写的一个系列专栏[《亿级流量系统架构演进》](https://link.juejin.im?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzU0OTk3ODQ3Ng%3D%3D%26mid%3D2247484026%26idx%3D1%26sn%3D32ebf21e0285d3947192d89fbe6a8cb7%26chksm%3Dfba6ea79ccd1636f8f9088f448b4a3942de57bf97f582de2feb93da06e4ddfca68a5a88befe0%26scene%3D21%26token%3D41464945%26lang%3Dzh_CN%23wechat_redirect)。

但是那么复杂的系统架构，通过一些文章是很难说的清楚里面的各种细节以及落地生产的过程的。

其次，高并发这话题本身包含的内容也远远不止本文说的这么几个topic：分库分表、缓存、消息。

一个完整而复杂的高并发系统架构中，一定会包含各种复杂的自研基础架构系统、各种精妙的架构设计（比如热点缓存架构设计、多优先级高吞吐MQ架构设计、系统全链路并发性能优化设计，等等）、还有各种复杂系统组合而成的高并发架构整体技术方案、还有NoSQL（Elasticsearch等）/负载均衡/Web服务器等相关技术。

所以大家切记要对技术保持敬畏之心，这些东西都很难通过一些文章来表述清楚。

最后，真正在生产落地的时候，高并发场景下你的系统会出现大量的技术问题。

比如说消息中间件吞吐量上不去需要优化、磁盘写压力过大性能太差、内存消耗过大容易撑爆、分库分表中间件不知道为什么丢了数据，等等吧。

诸如此类的问题非常多，这些也不可能通过文章给全部说清楚。

