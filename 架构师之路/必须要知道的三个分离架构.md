# 前言

如果你经历过创业，经历过快速迭代业务，经历过用户量不断上涨，经历过访问并发越来越大，你一定会遇到以下系统问题：

- 用户访问页面越来越**慢**
- 系统性能下降，**数据库扛不住**，连接数经常打满，最终数据库挂掉，重启后又快速挂掉
- 改了一个小地方，另外一个看似不相干的地方却挂了，**严重耦合**

 

如果你没有经历过，很可能是：

- 没到这一步项目就死了
- 身在所谓的大公司，用着所谓先进的架构体系

 

创业初期遇到上述痛点，很容易想到“三个分离”的架构优化方案：

- **前后分离**：前台与后台的数据与访问分离
- **动静分离**：能够100倍以上的提升静态页面/资源的访问速度
- **读写分离**：能够快速的线性扩充数据库的读性能



# 1  必备  前台与后台分离的架构实践

## 1.1 业务场景介绍

虚拟一个类似于“安居客”租房买房的业务场景，这个业务的**数据有两大来源**：

- 用户发布的数据
- 爬虫从竞对抓取来的数据

 

这个业务对应的系统有**两类使用者**：

- 普通用户，浏览与发布数据，俗称“前台用户”
- 后台用户，运营与管理数据，俗称“后台用户”

![image-20190708094437401](http://ww3.sinaimg.cn/large/006tNc79ly1g4sgwja3o4j30aq05bt9s.jpg)

在一个创业公司，为了快速迭代，系统架构如上：

- **web层**：前台web，后台web
- **任务层**：抓取数据
- **数据层**：存储数据



## 1.2 数据耦合的问题

系统两类数据源，一类是用户发布的数据，一类是爬虫抓取的数据，两类**数据的特点不一样**：

- **自有**数据相对结构化，变化少
- **抓取**数据源很多，数据结构变化快

 

如果将自有数据和抓取数据耦合在一个库里，经常出现的情况是：

- -> 抓取数据结构变化
- -> 需要修改数据结构
- -> 影响前台用户展现
- -> 经常被动修改前台用户展现逻辑，配合抓取升级

如果经历过这个过程，其中的痛不欲生，是谁都不愿意再次回忆起的。



**优化思路**：前台展现数据，后台抓取数据分离，解耦。

![image-20190708094548673](http://ww2.sinaimg.cn/large/006tNc79ly1g4sgwjsxtej30bm04q3zf.jpg)

如上图所示：

- 前台展现的稳定数据，库独立
- 后台抓取的多变数据，库独立
- 任务层新增一个异步转换的任务

 

如此这般：

- 频繁变化的抓取程序，以及抓取的异构数据存储，解耦
- 前台数据与web都不需要被动配合升级
- 即使出现问题，前台用户的发布与展现都不影响



## 1.3 系统耦合的问题

上面解决了不同数据源写入的耦合问题，再来看看前台与后台用户访问的耦合问题。

 

**用户侧**，前台访问的**特点**是：

- 访问模式有限
- 访问量较大，DAU不达到百万都不好意思说是互联网C端产品
- 对访问时延敏感，用户如果访问慢，立马就流失了
- 对服务可用性要求高，系统经常用不了，用户还会再来么
- 对数据一致性的要求高，关乎用户体验的事情就是大事

 

**运营侧**，后台访问的**特点**是：

- 访问模式多种多样，运营销售各种奇形怪状的，大批量分页的，查询需求
- 用户量小，访问量小
- 访问延时不这么敏感，大批量分页，几十秒能出结果，也能接受
- 对可用性能容忍，系统挂了，10分钟之内重启能回复，也能接受
- 对一致性的要求始终，晚个30秒的数据，也能接受

![image-20190708094653470](http://ww4.sinaimg.cn/large/006tNc79ly1g4sgwkaex3j307504hmxg.jpg)

前台和后台的模式与访问需求都不一样，但是，如果前台与后台混用同一套服务和结构化数据，会导致：

- 后台的低性能访问，对前台用户产生巨大的影响，本质还是耦合

![image-20190708094713596](http://ww1.sinaimg.cn/large/006tNc79ly1g4sgwkpkjrj309a04gdge.jpg)

- 随着数据量变大，为了保证前台用户的时延，质量，做一些类似与分库分表的升级，数据库一旦变化，可能很多后台的需求难以满足

 

**优化思路**：冗余数据，前台与后台服务与数据分离，解耦。

![image-20190708094732950](http://ww3.sinaimg.cn/large/006tNc79ly1g4sgwlb3i9j30d704nab4.jpg)

如上图所示：

- 前台和后台独立服务与数据，解耦
- 如果出现问题，相互不影响

![image-20190708094823371](http://ww2.sinaimg.cn/large/006tNc79ly1g4sgwm702cj30du04kwfa.jpg)

通过不同的技术方案，在不同容忍度，业务对系统要求不同的情况下，可以使用不同的技术栈来满足各自的需求，如上图，后台使用ES或者hive在进行数据存储，用以满足“售各种奇形怪状的，大批量分页的，查询需求”



## 1.4 总结

创业初期，快速实施架构优化，提升性能的“三大分离”优化利器：

- **动静分离**：能够100倍以上的提升静态页面/资源的访问速度
- **读写分离**：能够快速的线性扩充数据库的读性能
- **前后分离**：前台与后台的数据与访问分离



备注：另外一种[前后端分离](./架构师之路/分层架构.md#4--互联网分层架构为啥要前后端分离)



# 2  必备  动静分离架构实践

## 2.1 静态页面

静态页面，是指互联网架构中，几乎不变的页面(或者变化频率很低)，例如：

- 首页等html页面
- js/css等样式文件
- jpg/apk等资源文件

![image-20190708095651421](http://ww3.sinaimg.cn/large/006tNc79ly1g4sgwmkyuyj309702n0t1.jpg)

静态页面，有与之匹配的技术架构来加速，例如：

- CDN
- nginx
- squid/varnish



## 2.2 动态页面

动态页面，是指互联网架构中，不同用户不同场景访问，都不一样的页面，例如：

- 百度搜索结果页
- 淘宝商品列表页
- 速运个人订单中心页

这些页面，不同用户，不同场景访问，大都会动态生成不同的页面。

![image-20190708100041357](http://ww2.sinaimg.cn/large/006tNc79ly1g4sgwn6ulzj306404a0t8.jpg)

动态页面，有与之匹配的技术架构，例如：

- 分层架构
- 服务化架构
- 数据库，缓存架构

 

## 2.3 互联网动静分离架构

动静分离是指，静态页面与动态页面分开不同系统访问的架构设计方法。

![image-20190708100206505](http://ww4.sinaimg.cn/large/006tNc79ly1g4sgwnjdzej30dv059myc.jpg)

一般来说：

- **静态页面**访问路径短，访问速度快，几毫秒
- **动态页面**访问路径长，访问速度相对较慢(数据库的访问，网络传输，业务逻辑计算)，几十毫秒甚至几百毫秒，对架构扩展性的要求更高
- 静态页面与动态页面以不同域名区分



## 2.4 页面静态化

既然静态页面访问快，动态页面生成慢，有没有可能，将原本需要动态生成的站点提前生成好，使用静态页面加速技术来访问呢？

这就是互联网架构中的**“页面静态化”**优化技术。



举例，如下图，58同城的帖子详情页，原本是需要动态生成的：

![image-20190708100552790](http://ww4.sinaimg.cn/large/006tNc79ly1g4sgwo247kj30a906ign5.jpg)

- 浏览器发起http请求，访问/detail/12348888x.shtml **详情页**
- web-server层从RESTful接口中，解析出帖子id是**12348888**
- service层通过DAO层拼装**SQL语句**，访问数据库
- 最终获取数据，**拼装html返回浏览器**



而“页面静态化”是指，将帖子ID为12348888的帖子12348888x.shtml提前生成好，由静态页面相关加速技术来加速：

![image-20190708100616283](http://ww1.sinaimg.cn/large/006tNc79ly1g4sgwoxfl7j30c703h0ti.jpg)

这样的话，将极大提升访问速度，减少访问时间，提高用户体验。



## 2.5 页面静态化的应用场景

页面静态化优化后速度会加快，那能不能所有的场景都使用这个优化呢？**哪些业务场景适合使用这个架构优化方案呢？**

 

**一切脱离业务的架构设计都是耍流氓**，页面静态化，适用于：总数据量不大，生成静态页面数量不多的业务。例如：

- 58速运的城市页只有几百个，就可以用这个优化，只需提前生成几百个城市的“静态化页面”即可
- 一些二手车业务，只有几万量二手车库存，也可以提前生成这几万量二手车的静态页面
- 像58同城这样的信息模式业务，有几十亿的帖子量，就**不**太适合于静态化（碎片文件多，反而访问慢）



## 2.6 总结

**“页面静态化”**是一种将原本需要动态生成的站点提前生成静态站点的优化技术。

总数据量不大，**生成静态页面数量不多的业务**，非常适合于“页面静态化”优化。



# 3  必备  读写分离架构实践

## 3.1 什么是读写分离

**什么是数据库读写分离？**

![image-20190708150431176](http://ww2.sinaimg.cn/large/006tNc79ly1g4sgwpxtpzj306w053gmj.jpg)

答：一主多从，读写分离，主动同步，是一种常见的数据库架构，一般来说：

- 主库，提供数据库写服务
- 从库，提供数据库读服务
- 主从之间，通过某种机制同步数据，例如mysql的binlog

一个组从同步集群通常称为一个**“分组”**。



**分组架构究竟解决什么问题？**

答：大部分互联网业务读多写少，数据库的读往往最先成为性能瓶颈，如果希望：

- 线性提升数据库读性能
- 通过消除读写锁冲突提升数据库写性能

此时可以使用分组架构。

 

一句话，分组主要解决“数据库读性能瓶颈”问题，在数据库扛不住读的时候，通常读写分离，通过增加从库线性提升系统读性能。



## 3.2 什么是数据库水平切分

**什么是数据库水平切分？**

![image-20190708151007615](http://ww2.sinaimg.cn/large/006tNc79ly1g4sgwqbe16j3071031dg8.jpg)

答：水平切分，也是一种常见的数据库架构，一般来说：

- 每个数据库之间没有数据重合，没有类似binlog同步的关联
- 所有数据并集，组成全部数据
- 会用算法，来完成数据分割，例如“取模”

一个水平切分集群中的每一个数据库，通常称为一个**“分片”**。



在数据量很大的情况下，数据层（缓存，数据库）涉及数据的水平扩展，将原本存储在一台服务器上的数据（缓存，数据库）水平拆分到不同服务器上去，以达到扩充系统性能的目的。

 

互联网数据层常见的水平拆分方式有这么几种，以数据库为例：

### 3.2.1 按照范围水平拆分

![image-20190708150724192](http://ww4.sinaimg.cn/large/006tNc79ly1g4sgwqujb5j3085039jrt.jpg)

每一个数据服务，存储一定范围的数据，上图为例：

- user0库，存储uid范围1-1kw
- user1库，存储uid范围1kw-2kw



这个方案的好处是：

（1）规则简单，service只需判断一下uid范围就能路由到对应的存储服务；

（2）数据均衡性较好；

（3）比较容易扩展，可以随时加一个uid[2kw,3kw]的数据服务；



不足是：

（1）请求的负载不一定均衡，一般来说，新注册的用户会比老用户更活跃，大range的服务请求压力会更大；

### 3.2.2 按照哈希水平拆分

![image-20190708150817619](http://ww3.sinaimg.cn/large/006tNc79ly1g4sgwr9gbjj308e0310t3.jpg)

每一个数据库，存储某个key值hash后的部分数据，上图为例：

- user0库，存储偶数uid数据
- user1库，存储奇数uid数据



这个方案的好处是：

（1）规则简单，service只需对uid进行hash能路由到对应的存储服务；

（2）数据均衡性较好；

（3）请求均匀性较好；



不足是：

（1）不容易扩展，扩展一个数据服务，hash方法改变时候，可能需要进行数据迁移；



## 3.3 分库分表和读写分离的案例分析

首先我们先考虑第一个问题，数据库每秒上万的并发请求应该如何来支撑呢？

要搞清楚这个问题，先得明白一般数据库部署在什么配置的服务器上。通常来说，假如你用普通配置的服务器来部署数据库，那也起码是 16 核 32G 的机器配置。

这种非常普通的机器配置部署的数据库，**一般线上的经验是：不要让其每秒请求支撑超过 2000，一般控制在 2000 左右。**

控制在这个程度，一般数据库负载相对合理，不会带来太大的压力，没有太大的宕机风险。

所以首先第一步，就是在上万并发请求的场景下，部署个 5 台服务器，每台服务器上都部署一个数据库实例。

然后每个数据库实例里，都创建一个一样的库，比如说订单库。此时在 5 台服务器上都有一个订单库，名字可以类似为：db_order_01、db_order_02 等等。

然后每个订单库里，都有一个相同的表，比如说订单库里有订单信息表，那么此时 5 个订单库里都有一个订单信息表。

比如：db_order_01 库里就有一个 tb_order_01 表，db_order_02 库里就有一个 tb_order_02 表。

这就实现了一个基本的分库分表的思路，原来的一台数据库服务器变成了 5 台数据库服务器，原来的一个库变成了 5 个库，原来的一张表变成了 5 个表。

然后你在写入数据的时候，需要借助数据库中间件，比如 Sharding-JDBC，或者是 MyCAT，都可以。

你可以根据比如订单 ID 来 Hash 后按 5 取模，比如每天订单表新增 50 万数据，此时其中 10 万条数据会落入 db_order_01 库的 tb_order_01 表，另外 10 万条数据会落入 db_order_02 库的 tb_order_02 表，以此类推。

这样就可以把数据均匀分散在 5 台服务器上了，查询的时候，也可以通过订单ID 来 hash 取模，去对应的服务器上的数据库里，从对应的表里查询那条数据出来即可。



依据这个思路画出的图如下所示，大家可以看看：

![image-20190708151435919](http://ww4.sinaimg.cn/large/006tNc79ly1g4sgwrpse3j30i4073whv.jpg)

做这一步有什么好处呢？第一个好处，原来比如订单表就一张表，这个时候不就成了 5 张表了么，那么每个表的数据就变成 1/5 了。

假设订单表一年有 1 亿条数据，此时 5 张表里每张表一年就 2000 万数据了。

那么假设当前订单表里已经有 2000 万数据了，此时做了上述拆分，每个表里就只有 400 万数据了。

而且每天新增 50 万数据的话，那么每个表才新增 10 万数据，这样是不是初步缓解了单表数据量过大影响系统性能的问题？

另外就是每秒 1 万请求到 5 台数据库上，每台数据库就承载每秒 2000 的请求，是不是一下子把每台数据库服务器的并发请求降低到了安全范围内？

这样，降低了数据库的高峰期负载，同时还保证了高峰期的性能。



**大量分表来保证海量数据下的查询性能**
但是上述的数据库架构还有一个问题，那就是单表数据量还是过大，现在订单表才分为了 5 张表，那么如果订单一年有 1 亿条，每个表就有 2000 万条，这也还是太大了。

所以还应该继续分表，大量分表。比如可以把订单表一共拆分为 1024 张表，这样 1 亿数据量的话，分散到每个表里也就才 10 万量级的数据量，然后这上千张表分散在 5 台数据库里就可以了。

在写入数据的时候，需要做两次路由，先对订单 ID Hash 后对数据库的数量取模，可以路由到一台数据库上，然后再对那台数据库上的表数量取模，就可以路由到数据库上的一个表里了。

通过这个步骤，就可以让每个表里的数据量非常小，每年 1 亿数据增长，但是到每个表里才 10 万条数据增长，这个系统运行 10 年，每个表里可能才百万级的数据量。

这样可以一次性为系统未来的运行做好充足的准备，看下面的图，一起来感受一下：

![image-20190708151515758](http://ww3.sinaimg.cn/large/006tNc79ly1g4sgws9b4lj30jj08w787.jpg)



**读写分离来支撑按需扩容以及性能提升**
这个时候整体效果已经挺不错了，大量分表的策略保证可能未来 10 年，每个表的数据量都不会太大，这可以保证单表内的 SQL 执行效率和性能。

然后多台数据库的拆分方式，可以保证每台数据库服务器承载一部分的读写请求，降低每台服务器的负载。

但是此时还有一个问题，假如说每台数据库服务器承载每秒 2000 的请求，然后其中 400 请求是写入，1600 请求是查询。

也就是说，增删改的 SQL 才占到了 20% 的比例，80% 的请求是查询。此时假如说随着用户量越来越大，又变成每台服务器承载 4000 请求了。

那么其中 800 请求是写入，3200 请求是查询，如果说你按照目前的情况来扩容，就需要增加一台数据库服务器。

但是此时可能就会涉及到表的迁移，因为需要迁移一部分表到新的数据库服务器上去，是不是很麻烦？

其实完全没必要，数据库一般都支持读写分离，也就是做主从架构。

写入的时候写入主数据库服务器，查询的时候读取从数据库服务器，就可以让一个表的读写请求分开落地到不同的数据库上去执行。

这样的话，假如写入主库的请求是每秒 400，查询从库的请求是每秒 1600。

那么图大概如下所示：

![image-20190708151825873](http://ww4.sinaimg.cn/large/006tNc79ly1g4sgwsopujj30jo0epagv.jpg)

写入主库的时候，会自动同步数据到从库上去，保证主库和从库数据一致。

然后查询的时候都是走从库去查询的，这就通过数据库的主从架构实现了读写分离的效果了。

现在的好处就是，假如说现在主库写请求增加到 800，这个无所谓，不需要扩容。然后从库的读请求增加到了 3200，需要扩容了。

这时，你直接给主库再挂载一个新的从库就可以了，两个从库，每个从库支撑 1600 的读请求，不需要因为读请求增长来扩容主库。

实际上线上生产你会发现，读请求的增长速度远远高于写请求，所以读写分离之后，大部分时候就是扩容从库支撑更高的读请求就可以了。

而且另外一点，对同一个表，如果你既写入数据（涉及加锁），还从该表查询数据，可能会牵扯到锁冲突等问题，无论是写性能还是读性能，都会有影响。

所以一旦读写分离之后，对主库的表就仅仅是写入，没任何查询会影响他，对从库的表就仅仅是查询。



## 3.4 分库分表后如何生成全局唯一ID

在分库分表之后你必然要面对的一个问题，就是 id 咋生成？因为要是一个表分成多个表之后，每个表的 id 都是从 1 开始累加自增长，那肯定不对啊。

举个例子，你的订单表拆分为了 1024 张订单表，每个表的 id 都从 1 开始累加，这个肯定有问题了！

你的系统就没办法根据表主键来查询订单了，比如 id = 50 这个订单，在每个表里都有！

所以此时就需要分布式架构下的全局唯一 id 生成的方案了，在分库分表之后，对于插入数据库中的核心 id，不能直接简单使用表自增 id，要全局生成唯一 id，然后插入各个表中，保证每个表内的某个 id，全局唯一。

比如说订单表虽然拆分为了 1024 张表，但是 id = 50 这个订单，只会存在于一个表里。

那么如何实现全局唯一 id 呢？有以下几种方案：

### 3.4.1 方案一独立数据库自增 id

这个方案就是说你的系统每次要生成一个 id，都是往一个独立库的一个独立表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。

比如说你有一个 auto_id 库，里面就一个表，叫做 auto_id 表，有一个 id 是自增长的。

那么你每次要获取一个全局唯一 id，直接往这个表里插入一条记录，获取一个全局唯一 id 即可，然后这个全局唯一 id 就可以插入订单的分库分表中。

这个方案的好处就是方便简单，谁都会用。缺点就是单库生成自增 id，要是高并发的话，就会有瓶颈的，因为 auto_id 库要是承载个每秒几万并发，肯定是不现实的了。

### 3.4.2 UUID

这个每个人都应该知道吧，就是用 UUID 生成一个全局唯一的 id。

好处就是每个系统本地生成，不要基于数据库来了。不好之处就是，UUID 太长了，作为主键性能太差了，不适合用于主键。

如果你是要随机生成个什么文件名了，编号之类的，你可以用 UUID，但是作为主键是不能用 UUID 的。

### 3.4.3 获取系统当前时间

这个方案的意思就是获取当前时间作为全局唯一的 id。但是问题是，并发很高的时候，比如一秒并发几千，会有重复的情况，这个肯定是不合适的。

一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个 id，如果业务上你觉得可以接受，那么也是可以的。

你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号，比如说订单编号：时间戳 + 用户 id + 业务含义编码。

### 3.4.4 SnowFlake 算法的思想分析

SnowFlake 算法，是 Twitter 开源的分布式 id 生成算法。其核心思想就是：使用一个 64 bit 的 long 型的数字作为全局唯一 id。

这 64 个 bit 中，其中 1 个 bit 是不用的，然后用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id，12 bit 作为序列号。

![image-20190708152912751](http://ww3.sinaimg.cn/large/006tNc79ly1g4sgwtolmlj30jy04x74s.jpg)

给大家举个例子吧，比如下面那个 64 bit 的 long 型数字：

- 第一个部分，是 1 个 bit：0，这个是无意义的。 
- 第二个部分是 41 个 bit：表示的是时间戳。
- 第三个部分是 5 个 bit：表示的是机房 id，10001。
- 第四个部分是 5 个 bit：表示的是机器 id，1 1001。
- 第五个部分是 12 个 bit：表示的序号，就是某个机房某台机器上这一毫秒内同时生成的 id 的序号，0000 00000000。

**①1 bit：是不用的，为啥呢？**

因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。

**②41 bit：表示的是时间戳，单位是毫秒。**

41 bit 可以表示的数字多达 2^41 - 1，也就是可以标识 2 ^ 41 - 1 个毫秒值，换算成年就是表示 69 年的时间。

**③10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10 台机器上，也就是 1024 台机器。**

但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 2 ^ 5 个机房（32 个机房），每个机房里可以代表 2 ^ 5 个机器（32 台机器）。

**④12 bit：这个是用来记录同一个毫秒内产生的不同 id。**

12 bit 可以代表的最大正整数是 2 ^ 12 - 1 = 4096，也就是说可以用这个 12 bit 代表的数字来区分同一个毫秒内的 4096 个不同的 id。

简单来说，你的某个服务假设要生成一个全局唯一 id，那么就可以发送一个请求给部署了 SnowFlake 算法的系统，由这个 SnowFlake 算法系统来生成唯一 id。

这个 SnowFlake 算法系统首先肯定是知道自己所在的机房和机器的，比如机房 id = 17，机器 id = 12。

接着 SnowFlake 算法系统接收到这个请求之后，首先就会用二进制位运算的方式生成一个 64 bit 的 long 型 id，64 个 bit 中的第一个 bit 是无意义的。

接着 41 个 bit，就可以用当前时间戳（单位到毫秒），然后接着 5 个 bit 设置上这个机房 id，还有 5 个 bit 设置上机器 id。

最后再判断一下，当前这台机房的这台机器上这一毫秒内，这是第几个请求，给这次生成 id 的请求累加一个序号，作为最后的 12 个 bit。

最终一个 64 个 bit 的 id 就出来了，类似于：

![image-20190708152930908](http://ww1.sinaimg.cn/large/006tNc79ly1g4sgwu5pjuj30jw04w0t8.jpg)

这个算法可以保证说，一个机房的一台机器上，在同一毫秒内，生成了一个唯一的 id。可能一个毫秒内会生成多个 id，但是有最后 12 个 bit 的序号来区分开来。

下面我们简单看看这个 SnowFlake 算法的一个代码实现，这就是个示例，大家如果理解了这个意思之后，以后可以自己尝试改造这个算法。

总之就是用一个 64 bit 的数字中各个 bit 位来设置不同的标志位，区分每一个 id。

SnowFlake 算法的实现代码如下：

```java
public class IdWorker {
  private long workerId; // 这个就是代表了机器id
  private long datacenterId; // 这个就是代表了机房id
  private long sequence; // 这个就是代表了一毫秒内生成的多个id的最新序号
  public IdWorker(long workerId, long datacenterId, long sequence) {
    // sanity check for workerId
    // 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0
    if (workerId > maxWorkerId || workerId < 0) {

      throw new IllegalArgumentException(
        String.format("worker Id can't be greater than %d or less than 0",maxWorkerId));
    }

    if (datacenterId > maxDatacenterId || datacenterId < 0) {

      throw new IllegalArgumentException(
        String.format("datacenter Id can't be greater than %d or less than 0",maxDatacenterId));
    }
    this.workerId = workerId;
    this.datacenterId = datacenterId;
    this.sequence = sequence;
  }
  private long twepoch = 1288834974657L;
  private long workerIdBits = 5L;
  private long datacenterIdBits = 5L;

  // 这个是二进制运算，就是5 bit最多只能有31个数字，也就是说机器id最多只能是32以内
  private long maxWorkerId = -1L ^ (-1L << workerIdBits); 
  // 这个是一个意思，就是5 bit最多只能有31个数字，机房id最多只能是32以内
  private long maxDatacenterId = -1L ^ (-1L << datacenterIdBits); 
  private long sequenceBits = 12L;
  private long workerIdShift = sequenceBits;
  private long datacenterIdShift = sequenceBits + workerIdBits;
  private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits;
  private long sequenceMask = -1L ^ (-1L << sequenceBits);
  private long lastTimestamp = -1L;
  public long getWorkerId(){
    return workerId;
  }
  public long getDatacenterId() {
    return datacenterId;
  }
  public long getTimestamp() {
    return System.currentTimeMillis();
  }
  // 这个是核心方法，通过调用nextId()方法，让当前这台机器上的snowflake算法程序生成一个全局唯一的id
  public synchronized long nextId() {
    // 这儿就是获取当前时间戳，单位是毫秒
    long timestamp = timeGen();
    if (timestamp < lastTimestamp) {
      System.err.printf(
        "clock is moving backwards. Rejecting requests until %d.", lastTimestamp);
      throw new RuntimeException(
        String.format("Clock moved backwards. Refusing to generate id for %d milliseconds",
               lastTimestamp - timestamp));
    }

    // 下面是说假设在同一个毫秒内，又发送了一个请求生成一个id
    // 这个时候就得把seqence序号给递增1，最多就是4096
    if (lastTimestamp == timestamp) {

      // 这个意思是说一个毫秒内最多只能有4096个数字，无论你传递多少进来，
      //这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围
      sequence = (sequence + 1) & sequenceMask; 
      if (sequence == 0) {
        timestamp = tilNextMillis(lastTimestamp);
      }

    } else {
      sequence = 0;
    }
    // 这儿记录一下最近一次生成id的时间戳，单位是毫秒
    lastTimestamp = timestamp;
    // 这儿就是最核心的二进制位运算操作，生成一个64bit的id
    // 先将当前时间戳左移，放到41 bit那儿；将机房id左移放到5 bit那儿；将机器id左移放到5 bit那儿；将序号放最后12 bit
    // 最后拼接起来成一个64 bit的二进制数字，转换成10进制就是个long型
    return ((timestamp - twepoch) << timestampLeftShift) |
        (datacenterId << datacenterIdShift) |
        (workerId << workerIdShift) | sequence;
  }
  private long tilNextMillis(long lastTimestamp) {

    long timestamp = timeGen();

    while (timestamp <= lastTimestamp) {
      timestamp = timeGen();
    }
    return timestamp;
  }
  private long timeGen(){
    return System.currentTimeMillis();
  }
  //---------------测试---------------
  public static void main(String[] args) {

    IdWorker worker = new IdWorker(1,1,1);

    for (int i = 0; i < 30; i++) {
      System.out.println(worker.nextId());
    }
  }
}
```

SnowFlake 算法一个小小的改进思路：其实在实际的开发中，这个SnowFlake算法可以做一点点改进。因为大家可以考虑一下，我们在生成唯一 id 的时候，一般都需要指定一个表名，比如说订单表的唯一 id。

所以上面那 64 个 bit 中，代表机房的那 5 个 bit，可以使用业务表名称来替代，比如用 00001 代表的是订单表。

因为其实很多时候，机房并没有那么多，所以那 5 个 bit 用做机房 id 可能意义不是太大。

这样就可以做到，SnowFlake 算法系统的每一台机器，对一个业务表，在某一毫秒内，可以生成一个唯一的 id，一毫秒内生成很多 id，用最后 12 个 bit 来区分序号对待。



## 3.5 你的系统如何支撑高并发





