# Table of Contents

* [1  基本概念](#1--基本概念)
  * [1.1 概念一“单库”](#11-概念一单库)
  * [1.2 概念二“分片”](#12-概念二分片)
  * [1.3 概念三“分组”](#13-概念三分组)
* [2  数据库架构设计思路](#2--数据库架构设计思路)
  * [2.1 如何保证数据的可用性](#21-如何保证数据的可用性)
  * [2.2 如何扩展读性能](#22-如何扩展读性能)
  * [2.3 如何保证一致性](#23-如何保证一致性)
  * [2.4 如何提高数据库的扩展性](#24-如何提高数据库的扩展性)
    * [2.4.1 缘起](#241-缘起)
    * [2.4.2 停服务方案](#242-停服务方案)
    * [2.4.3 秒级 平滑 帅气方案](#243-秒级-平滑-帅气方案)
    * [2.4.4 总结](#244-总结)


# 1  基本概念

## 1.1 概念一“单库”

![image-20190710164007732](http://ww3.sinaimg.cn/large/006tNc79ly1g4vlv736kuj303s01d745.jpg)

## 1.2 概念二“分片”

![image-20190710164020142](http://ww3.sinaimg.cn/large/006tNc79ly1g4vlv7ix6gj304a01dmx3.jpg)

**分片解决的是“数据量太大”的问题**，也就是通常说的“水平切分”。

一旦引入分片，势必有“数据路由”的概念，哪个数据访问哪个库。



路由规则通常有3种方法：

（1）**范围**：range

优点：简单，容易扩展

缺点：各库压力不均（新号段更活跃）

（2）**哈希**：hash

优点：简单，数据均衡，负载均匀

缺点：迁移麻烦（2库扩3库数据要迁移）

（3）**路由服务**：router-config-server

优点：灵活性强，业务与路由算法解耦

缺点：每次访问数据库前多一次查询



大部分互联网公司采用的方案二：哈希分库，哈希路由



## 1.3 概念三“分组”

![image-20190710164208751](http://ww3.sinaimg.cn/large/006tNc79ly1g4vlv8fm29j305r02kmx7.jpg)

**分组解决“可用性”问题**，分组通常通过主从复制的方式实现。



**互联网公司数据库实际软件架构是：又分片，又分组（如下图）**

![image-20190710164242881](http://ww3.sinaimg.cn/large/006tNc79ly1g4vlv9g9vgj308n02z74s.jpg)



# 2  数据库架构设计思路

数据库软件架构师平时设计些什么东西呢？至少要考虑以下四点：

（1）如何保证数据可用性

（2）如何提高数据库读性能（大部分应用读多写少，读会先成为瓶颈）

（3）如何保证一致性

（4）如何提高扩展性

## 2.1 如何保证数据的可用性

如何保证站点的可用性？复制站点，冗余站点

如何保证服务的可用性？复制服务，冗余服务

如何保证数据的可用性？复制数据，冗余数据

数据的冗余，会带来一个副作用=>引发一致性问题（先不说一致性问题，先说可用性）

**如何保证数据库“读”高可用？**

冗余读库

![image-20190710164553315](http://ww3.sinaimg.cn/large/006tNc79ly1g4vlv9wh65j305t047wep.jpg)

冗余读库带来的副作用？读写有延时，可能不一致

上面这个图是很多互联网公司mysql的架构，写仍然是单点，不能保证写高可用。

**如何保证数据库“写”高可用？**

冗余写库

![image-20190710164710178](http://ww1.sinaimg.cn/large/006tNc79ly1g4vlvab1yij306w0450t3.jpg)

采用双主互备的方式，可以冗余写库

带来的副作用？双写同步，数据可能冲突（例如“自增id”同步冲突）,如何解决同步冲突，有两种常见解决方案：

（1）两个写库使用不同的初始值，相同的步长来增加id：1写库的id为0,2,4,6...；2写库的id为1,3,5,7…

（2）不使用数据的id，业务层自己生成唯一的id，保证数据不冲突



58同城没有使用上述两种架构来做读写的“高可用”，58同城采用的是“双主当主从用”的方式：

![image-20190710164756115](http://ww2.sinaimg.cn/large/006tNc79ly1g4vlvaw6sxj303d03bglk.jpg)

仍是双主，但只有一个主提供服务（读+写），另一个主是“shadow-master”，只用来保证高可用，平时不提供服务。

master挂了，shadow-master顶上（vip漂移，对业务层透明，不需要人工介入）

这种方式的**好处**：

1）读写没有延时

2）读写高可用

**不足**：

1）不能通过加从库的方式扩展读性能

2）资源利用率为50%，一台冗余主没有提供服务



## 2.2 如何扩展读性能

提高读性能的方式大致有三种，第一种是**建立索引**。这种方式不展开，要提到的一点是，不同的库可以建立不同的索引。

![image-20190710165624760](http://ww2.sinaimg.cn/large/006tNc79ly1g4vlvbb949j3082041mxk.jpg)

**写库**不建立索引；

**线上读库**建立线上访问索引，例如uid；

**线下读库**建立线下访问索引，例如time；



第二种扩充读性能的方式是，**增加从库**，这种方法大家用的比较多，但是，存在两个缺点：

（1）从库越多，同步越慢

（2）同步越慢，数据不一致窗口越大（不一致后面说，还是先说读性能的提高）



58同城没有采用这种方法提高数据库读性能（没有从库），采用的是**增加缓存**。常见的缓存架构如下：

![image-20190710165723246](http://ww1.sinaimg.cn/large/006tNc79ly1g4vlvc8xg3j308304hq3h.jpg)

上游是业务应用，下游是主库，从库（读写分离），缓存。

58同城的玩法是：**服务+数据库+缓存一套**

![image-20190710165755604](http://ww2.sinaimg.cn/large/006tNc79ly1g4vlvcoqljj305z06l0te.jpg)

业务层不直接面向db和cache，服务层屏蔽了底层db、cache的复杂性。58采用了“服务+数据库+缓存一套”的方式提供数据访问，用cache提高读性能。

不管采用主从的方式扩展读性能，还是缓存的方式扩展读性能，数据都要复制多份（主+从，db+cache），一定会引发一致性问题。



## 2.3 如何保证一致性

主从数据库的一致性，通常有两种解决方案：

**（1）中间件**

![image-20190710170253350](http://ww1.sinaimg.cn/large/006tNc79ly1g4vlvd8y7dj307y03wdgj.jpg)

如果某一个key有写操作，在不一致时间窗口内，中间件会将这个key的读操作也路由到主库上。

这个方案的缺点是，数据库中间件的门槛较高（百度，腾讯，阿里，360等一些公司有，当然58也有）

**（2）强制读主**

![image-20190710170304404](http://ww1.sinaimg.cn/large/006tNc79ly1g4vlvdnvbhj303o03edft.jpg)

58的“双主当主从用”的架构，不存在主从不一致的问题。



第二类不一致，是db与缓存间的不一致

![image-20190710170331853](http://ww1.sinaimg.cn/large/006tNc79ly1g4vlve1zboj307x047gm5.jpg)

常见的缓存架构如上，此时**写操作的顺序是**：

（1）淘汰cache

（2）写数据库

**读操作的顺序是**：

（1）读cache，如果cache hit则返回

（2）如果cache miss，则读从库

（3）读从库后，将数据放回cache



在一些异常时序情况下，有可能从【从库读到旧数据（同步还没有完成），旧数据入cache后】，数据会长期不一致。



db与缓存不一致，通常有两种解决方案

**（1）缓存双删策略+缓存超时设置**

在写库前后都进行redis.del(key)操作，并且设定合理的超时时间。

```java
public void write(String key,Object data){
	redis.delKey(key);
  db.updateData(data);
	Thread.sleep(500);
	redis.delKey(key);
}
```

**具体的步骤就是：**

- 先删除缓存；
- 再写数据库；
- 休眠500毫秒；
- 再次删除缓存。

那么，这个500毫秒怎么确定的，具体该休眠多久呢？

需要评估自己的项目的读数据业务逻辑的耗时。这么做的目的，在写请求的同时，有一个线程还在读取数据，然后将数据放回cache中，这样缓存中又出现脏数据。所以必须在写请求之后休眠一个时间，这个时间必须大于读数据的耗时。

当然这种策略还要考虑redis和数据库主从同步的耗时。最后的的写数据的休眠时间：则在读数据业务逻辑的耗时基础上，加几百ms即可。比如：休眠1秒。

**设置缓存超时时间**

从理论上来说，给缓存设置超时时间，是保证最终一致性的解决方案。所有的写操作以数据库为准，只要到达缓存超时时间，则后面的读请求自然会从数据库中读取新值然后回填缓存。

**该方案的弊端**

结合缓存双删策略+缓存超时设置，这样最差的情况就是在超时时间内数据存在不一致，而且又增加了写请求的耗时。

**（2）异步更新缓存(基于订阅binlog的同步机制)**

**技术整体思路：**
MySQL binlog增量订阅消息+消息队列+增量数据更新到redis
读Redis：热数据基本都在Redis
写MySQL:增删改都是操作MySQL
更新Redis数据：MySQ的数据操作binlog，来更新到Redis

**Redis更新**

1）数据操作主要分为两大块：

一个是全量(将全部数据一次写入到redis)
一个是增量（实时更新）
这里说的是增量,指的是mysql的update、insert、delate变更数据。

2）读取binlog后分析，利用消息队列,推送更新各台的redis缓存数据。

这样一旦MySQL中产生了新的写入、更新、删除等操作，就可以把binlog相关的消息推送至Redis，Redis再根据binlog中的记录，对Redis进行更新。

其实这种机制，很类似MySQL的主从备份机制，因为MySQL的主备也是通过binlog来实现的数据一致性。

这里可以结合使用canal(阿里的一款开源框架)，通过该框架可以对MySQL的binlog进行订阅，而canal正是模仿了mysql的slave数据库的备份请求，使得Redis的数据更新达到了相同的效果。



## 2.4 如何提高数据库的扩展性

原来用hash的方式路由，分为2个库，数据量还是太大，要分为3个库，势必需要进行数据迁移，58同城有一个很帅气的“数据库秒级扩容”方案。

我们不做2库变3库的扩容，我们做2库变4库（库加倍）的扩容（未来4->8->16），怎么操作呢

### 2.4.1 缘起

（1）并发量大，流量大的互联网架构，一般来说，数据库上层都有一个**服务层**，服务层记录了“业务库名”与“数据库实例”的映射关系，通过**数据库连接池**向数据库路由sql语句以执行：

![image-20190710183830001](http://ww3.sinaimg.cn/large/006tNc79ly1g4vlvenfpkj305u0310sw.jpg)

如上图：服务层配置用户库user对应的数据库实例物理位置为ip（其实是一个内网域名）。

 

（2）随着数据量的增大，数据要进行**水平切分**，分库后将数据分布到不同的数据库实例（甚至物理机器）上，以达到降低数据量，增强性能的扩容目的：

![image-20190710183858891](http://ww2.sinaimg.cn/large/006tNc79ly1g4vlvf2u7tj309a03774v.jpg)

如上图：用户库user分布在两个实例上，ip0和ip1，服务层通过用户标识uid取模的方式进行寻库路由，模2余0的访问ip0上的user库，模2余1的访问ip1上的user库。

（3）互联网架构需要保证**数据库高可用**，常见的一种方式，使用双主同步+keepalived+虚ip的方式保证数据库的可用性：

![image-20190710183927288](http://ww3.sinaimg.cn/large/006tNc79ly1g4vlvfhxpmj306r02zgly.jpg)

如上图：两个相互同步的主库使用相同的虚ip。

![image-20190710183941306](http://ww4.sinaimg.cn/large/006tNc79ly1g4vlvfzehjj306r02x3yz.jpg)

如上图：当主库挂掉的时候，虚ip自动漂移到另一个主库，**整个过程对调用方透明**，通过这种方式保证数据库的高可用。

（4）综合上文的（2）和（3），线上实际的架构，既有水平切分，又有高可用保证，所以实际的数据库架构是这样的：

![image-20190710184007839](http://ww2.sinaimg.cn/large/006tNc79ly1g4vlvgw6lpj30bm03h3zd.jpg)

**提问**：如果数据量持续增大，分2个库性能扛不住了，该怎么办呢？

**回答**：继续水平拆分，拆成更多的库，降低单库数据量，增加库主库实例（机器）数量，提高性能。



**最终问题抛出**：分成x个库后，随着数据量的增加，要增加到y个库，数据库扩容的过程中，能否平滑，持续对外提供服务，保证服务的可用性，是本文要讨论的问题。

### 2.4.2 停服务方案

在讨论平滑方案之前，先简要说明下“x库拆y库”停服务的方案：

（1）站点挂一个公告“为了为广大用户提供更好的服务，本站点/游戏将在今晚00:00-2:00之间升级，届时将不能登录，用户周知”

（2）停服务

（3）新建y个库，做好高可用

（4）数据迁移，重新分布，写一个数据迁移程序，从x个库里导入到y个库里，路由规则由%x升级为%y

（5）修改服务配置，原来x行配置升级为y行

（6）重启服务，连接新库重新对外提供服务

整个过程中，**最耗时的是第四步数据迁移**。

 

**回滚方案**：

如果数据迁移失败，或者迁移后测试失败，则将配置改回x库，恢复服务，改天再挂公告。

 

**方案优点**：简单

 

**方案缺点**：

（1）停服务，不高可用

（2）技术同学压力大，所有工作要在规定时间内做完，根据经验，压力越大约容易出错（这一点很致命）

（3）如果有问题第一时间没检查出来，启动了服务，运行一段时间后再发现有问题，难以回滚，需要回档，可能会丢失一部分数据

 

有没有更平滑的方案呢？

### 2.4.3 秒级 平滑 帅气方案

![image-20190710184125726](http://ww3.sinaimg.cn/large/006tNc79ly1g4vlvhfj9pj30ba036my0.jpg)

再次看一眼扩容前的架构，分两个库，假设每个库1亿数据量，如何**平滑扩容，增加实例数，降低单库数据量**呢？三个简单步骤搞定。

 

**（1）修改配置**

![image-20190710184143486](http://ww4.sinaimg.cn/large/006tNc79ly1g4vlvicy9nj30c204tq48.jpg)

主要修改两处：

a）数据库实例所在的机器做双虚ip，原来%2=0的库是虚ip0，现在增加一个虚ip00，%2=1的另一个库同理

b）修改服务的配置（不管是在配置文件里，还是在配置中心），将2个库的数据库配置，改为4个库的数据库配置，**修改的时候要注意旧库与新库的映射关系**：

%2=0的库，会变为%4=0与%4=2；

%2=1的部分，会变为%4=1与%4=3；

这样修改是为了保证，拆分后依然能够路由到正确的数据。

 

**（2）reload配置，实例扩容**

![image-20190710184211695](http://ww1.sinaimg.cn/large/006tNc79ly1g4vlvixh6jj30b705ftaj.jpg)

服务层reload配置，reload可能是这么几种方式：

a）比较原始的，重启服务，读新的配置文件

b）高级一点的，配置中心给服务发信号，重读配置文件，重新初始化数据库连接池



不管哪种方式，reload之后，数据库的**实例扩容就完成了**，原来是2个数据库实例提供服务，现在变为4个数据库实例提供服务，这个过程一般可以在秒级完成。

 

整个过程可以逐步重启，**对服务的正确性和可用性完全没有影响**：

a）即使%2寻库和%4寻库同时存在，也不影响数据的正确性，因为此时仍然是双主数据同步的

b）服务reload之前是不对外提供服务的，冗余的服务能够保证高可用

 

完成了实例的扩展，会发现每个数据库的数据量依然没有下降，所以第三个步骤还要做一些收尾工作。



**（3）收尾工作，数据收缩**

![image-20190710184308781](http://ww1.sinaimg.cn/large/006tNc79ly1g4vlvjc4x2j30bm06y76o.jpg)

有这些一些**收尾工作**：

a）把双虚ip修改回单虚ip

b）解除旧的双主同步，让成对库的数据不再同步增加

c）增加新的双主同步，保证高可用（双主是保证可用性的，shadow-master平时不提供服务）

d）删除掉冗余数据，例如：ip0里%4=2的数据全部干掉，只为%4=0的数据提供服务啦



这样下来，**每个库的数据量就降为原来的一半**，**数据收缩完成**。

### 2.4.4 总结

![image-20190710184354972](http://ww2.sinaimg.cn/large/006tNc79ly1g4vlvjpkqrj30ix05vgod.jpg)

该帅气方案能够实现n库扩2n库的秒级、平滑扩容，增加数据库服务能力，降低单库一半的数据量，其核心原理是：**成倍扩容，避免数据迁移**。



**迁移步骤**：

（1）修改配置

（2）reload配置，**实例扩容**完成

（3）删除冗余数据等收尾工作，**数据量收缩**完成
